{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernelized Synaptic Weight Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import os\n",
    "\n",
    "seed = int(time())\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `load_data`\n",
    "\n",
    "loads ml-1m data\n",
    "\n",
    "- :param path: path to the ratings file\n",
    "- :param val_frac: fraction of data to use for validation\n",
    "- :param delimiter: delimiter used in data file\n",
    "- :param seed: random seed for validation splitting\n",
    "- :param transpose: flag to transpose output matrices (swapping users with movies)\n",
    "- :return: train ratings (n_u, n_m), valid ratings (n_u, n_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(\n",
    "    path='./',\n",
    "    val_frac=0.1,\n",
    "    delimiter='::',\n",
    "    seed=1234,\n",
    "    transpose=False\n",
    "):\n",
    "    tic = time()\n",
    "    print('reading data...')\n",
    "\n",
    "    data = pd.read_csv(os.path.join(path, 'ratings.csv'), header=0)\n",
    "    print('data read in', time() - tic, 'seconds')\n",
    "\n",
    "    unique_user_ids = pd.unique(data['userId'])\n",
    "    unique_movie_ids = pd.unique(data['movieId'])\n",
    "\n",
    "    n_u = len(unique_user_ids)\n",
    "    n_m = len(unique_movie_ids)\n",
    "    n_r = len(data)\n",
    "\n",
    "    # these dictionaries define a mapping\n",
    "    # from user/movie id to to user/movie number (contiguous from zero)\n",
    "    udict = {}\n",
    "    for i, u in enumerate(unique_user_ids):\n",
    "        udict[u] = i\n",
    "\n",
    "    mdict = {}\n",
    "    for i, m in enumerate(unique_movie_ids):\n",
    "        mdict[m] = i\n",
    "\n",
    "    # shuffle indices\n",
    "    idx = np.arange(n_r)\n",
    "    np.random.shuffle(idx)\n",
    "\n",
    "    train_ratings = np.zeros((n_u, n_m), dtype='float32')\n",
    "    valid_ratings = np.zeros((n_u, n_m), dtype='float32')\n",
    "\n",
    "    for i in range(n_r):\n",
    "        u_id = data['userId'][i]\n",
    "        m_id = data['movieId'][i]\n",
    "        r = data['rating'][i]\n",
    "\n",
    "        if i <= val_frac * n_r:\n",
    "            valid_ratings[udict[u_id], mdict[m_id]] = int(r)\n",
    "        else:\n",
    "            train_ratings[udict[u_id], mdict[m_id]] = int(r)\n",
    "\n",
    "    if transpose:\n",
    "        train_ratings = train_ratings.T\n",
    "        valid_ratings = valid_ratings.T\n",
    "\n",
    "    return train_ratings, valid_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "tr, vr = load_data(\n",
    "    path='./dataset/small',\n",
    "    delimiter='::',\n",
    "    seed=seed,\n",
    "    transpose=True,\n",
    "    val_frac=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm = np.greater(tr, 1e-12).astype('float32')  # masks indicating non-zero entries\n",
    "vm = np.greater(vr, 1e-12).astype('float32')\n",
    "\n",
    "n_m = tr.shape[0]  # number of movies\n",
    "n_u = tr.shape[1]  # number of users (may be switched depending on 'transpose' in loadData)\n",
    "\n",
    "# Set hyper-parameters\n",
    "n_hid = 500\n",
    "lambda_2 = 60. # float(sys.argv[1]) if len(sys.argv) > 1 else 60.\n",
    "lambda_s = 0.013 # float(sys.argv[2]) if len(sys.argv) > 2 else 0.013\n",
    "n_layers = 2\n",
    "output_every = 50  # evaluate performance on test set; breaks l-bfgs loop\n",
    "n_epoch = n_layers * 10 * output_every\n",
    "verbose_bfgs = True\n",
    "use_gpu = False\n",
    "\n",
    "if not use_gpu:\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "\n",
    "# Input placeholders\n",
    "R = tf.placeholder(\"float\", [None, n_u])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `kernel`\n",
    "\n",
    "Sparsifying kernel function\n",
    "\n",
    "- :param u: input vectors `[n_in, 1, n_dim]`\n",
    "- :param v: output vectors `[1, n_hid, n_dim]`\n",
    "- :return: input to output connection matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel(u, v):\n",
    "    dist = tf.norm(u - v, ord=2, axis=2)\n",
    "    hat = tf.maximum(0., 1. - dist**2)\n",
    "    return hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `kernel_layer`\n",
    "\n",
    "a kernel sparsified layer\n",
    "\n",
    "- :param x: input `[batch, channels]`\n",
    "- :param n_hid: number of hidden units\n",
    "- :param n_dim: number of dimensions to embed for kernelization\n",
    "- :param activation: output activation\n",
    "- :param name: layer name for scoping\n",
    "- :return: layer output, regularization term\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_layer(\n",
    "    x,\n",
    "    n_hid=500,\n",
    "    n_dim=5,\n",
    "    activation=tf.nn.sigmoid,\n",
    "    lambda_s=lambda_s,\n",
    "    lambda_2=lambda_2,\n",
    "    name=''\n",
    "):\n",
    "    # define variables\n",
    "    with tf.variable_scope(name):\n",
    "        W = tf.get_variable('W', [x.shape[1], n_hid])\n",
    "        n_in = x.get_shape().as_list()[1]\n",
    "        u = tf.get_variable('u', initializer=tf.random.truncated_normal([n_in, 1, n_dim], 0., 1e-3))\n",
    "        v = tf.get_variable('v', initializer=tf.random.truncated_normal([1, n_hid, n_dim], 0., 1e-3))\n",
    "        b = tf.get_variable('b', [n_hid])\n",
    "\n",
    "    # compute sparsifying kernel\n",
    "    # as u and v move further from each other\n",
    "    # for some given pair of neurons, their connection\n",
    "    # decreases in strength and eventually goes to zero.\n",
    "    w_hat = kernel(u, v)\n",
    "\n",
    "    # compute regularization terms\n",
    "    sparse_reg = tf.contrib.layers.l2_regularizer(lambda_s)\n",
    "    sparse_reg_term = tf.contrib.layers.apply_regularization(sparse_reg, [w_hat])\n",
    "\n",
    "    l2_reg = tf.contrib.layers.l2_regularizer(lambda_2)\n",
    "    l2_reg_term = tf.contrib.layers.apply_regularization(l2_reg, [W])\n",
    "\n",
    "    # compute output\n",
    "    W_eff = W * w_hat\n",
    "    y = tf.matmul(x, W_eff) + b\n",
    "    y = activation(y)\n",
    "\n",
    "    return y, sparse_reg_term + l2_reg_term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 여기서부터 메인 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate network\n",
    "y = R\n",
    "reg_losses = None\n",
    "for i in range(n_layers):\n",
    "    y, reg_loss = kernel_layer(y, n_hid, name=str(i))\n",
    "    reg_losses = reg_loss if reg_losses is None else reg_losses + reg_loss\n",
    "\n",
    "prediction, reg_loss = kernel_layer(y, n_u, activation=tf.identity, name='out')\n",
    "reg_losses = reg_losses + reg_loss\n",
    "\n",
    "# Compute loss (symbolic)\n",
    "diff = tm*(R - prediction)\n",
    "sqE = tf.nn.l2_loss(diff)\n",
    "loss = sqE + reg_losses\n",
    "\n",
    "# Instantiate L-BFGS Optimizer\n",
    "optimizer = tf.contrib.opt.ScipyOptimizerInterface(\n",
    "    loss,\n",
    "    options={\n",
    "        'maxiter': output_every,\n",
    "        'disp': verbose_bfgs,\n",
    "        'maxcor': 10\n",
    "    },\n",
    "    method='L-BFGS-B'\n",
    ")\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for i in range(int(n_epoch / output_every)):\n",
    "        optimizer.minimize(sess, feed_dict={R: tr}) # do maxiter optimization steps\n",
    "        pre = sess.run(prediction, feed_dict={R: tr}) # predict ratings\n",
    "\n",
    "        # compute validation error\n",
    "        error = (vm * (np.clip(pre, 1., 5.) - vr) ** 2).sum() / vm.sum()\n",
    "\n",
    "        # compute train error\n",
    "        error_train = (tm * (np.clip(pre, 1., 5.) - tr) ** 2).sum() / tm.sum()\n",
    "\n",
    "        print('.-^-._' * 12)\n",
    "        print('epoch:', i, 'validation rmse:', np.sqrt(error), 'train rmse:', np.sqrt(error_train))\n",
    "        print('.-^-._' * 12)\n",
    "\n",
    "    with open('summary_ml1m.txt', 'a') as file:\n",
    "        for a in sys.argv[1:]:\n",
    "            file.write(a + ' ')\n",
    "\n",
    "        file.write(\n",
    "            str(np.sqrt(error)) + ' ' +\n",
    "            str(np.sqrt(error_train)) + ' ' +\n",
    "            str(seed) + '\\n'\n",
    "        )\n",
    "        file.close()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37664bitvenvvirtualenv62f0062b5c8b49d1bdab95e8e58e4689",
   "display_name": "Python 3.7.6 64-bit ('.venv': virtualenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}